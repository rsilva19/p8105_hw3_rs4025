---
title: "p8105_hw3_rs4025"
author: "Rebecca Silva"
date: "10/14/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

#scale_colour_discrete = scale_colour_viridis_d
#scale_fill_discrete = scale_fill_viridis_d
#theme_set(theme_minimal() + theme(legend.position = "bottom"))

library(tidyverse)
library(viridis)
library(p8105.datasets)
library(kableExtra)
#dont need below if dont have to read in prob 3 the way he does 
library(httr)
library(jsonlite)
library(patchwork) 
```

# Questions: 

+ delete/renumber tables 
+ plot for prob 1?
+ add labels and titles to all plots! and follow rubric 
+ is "patchwork" pkg already in one of my packages
+ still need to comment on findings of prob 2

## Problem 1 

# write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. 

The `instacart` dataset gives infomation about online grocery shopping in 2017 using the grocery service, Instacart. It has `r nrow(instacart)` observations where each observation represents a specific product from a specific order number. There are `r ncol(instacart)` variables, 4 of which are character variables, and 11 of which are integer variables. Some key variables and ones that we focus on are as follows: 

* `order_id`: order number 
* `product_id`: product number
* `user_id`: user number
* `order_number`: sequence number of order for user
* `order_dow`: day of the week on which order placed (0-7)
* `order_hour_of_day`: hour of day in which order placed 
* `product_name` (character): name of product
* `aisle_id`: aisle number
* `aisle` (character): sorts/types of products aisle contains 
* `department` (character): name of department/ product categories


# Q: give illstrative examples of observations ?

To give an example of the dataset, the first 5 observations are shown below in Table 1. 
```{r}
head = head(instacart, 5) 
knitr::kable(head, 
             caption = "Table 1: First 10 Observation of Instacart dataset")
```

# How many aisles are there, and which aisles are the most items ordered from?

```{r}
aisle = 
  instacart %>% 
  group_by(aisle) %>%   
  summarize(n = n()) %>% 
  arrange(desc(n))
```

There are `r nrow(aisle)` aisles in the online grocery store, the most popular of which are fresh vegetables (150609 total orders), fresh fruits (150473 total orders), and packaged vegetables and fruit (78493 total orders).


# Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

# Q: do not know what he wants (cld order by department? no)
```{r}

aisle %>% 
  ungroup() %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n)) %>% 
  filter( n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_bar(stat = "identit") + 
  labs(
    title = "Number of Items Ordered in the Most Popular Aisles",
    x = "Aisle",
    y = "Number of Items") +
  theme(axis.text.x = element_text(angle = 90),
        plot.title = element_text(hjust = 0.5))


# best one i think
# by department color
instacart %>% 
  group_by(department, aisle) %>% 
  summarize(n = n()) %>% 
  ungroup() %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n)) %>% 
  filter( n > 10000) %>% 
  ggplot(aes(x = aisle, y = n, fill = department)) + 
  geom_bar(stat = "identity") + 
  labs(
    title = "Number of Items Ordered in the Most Popular Aisles",
    x = "Aisle",
    y = "Number of Items") +
  theme(axis.text.x = element_text(angle = 90),
        plot.title = element_text(hjust = 0.5))

```

# Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r}
table_aisle = 
  instacart %>% 
  filter(aisle %in% c("baking ingredients",
                      "dog food care", 
                      "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>% 
  summarise(n = n()) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>% 
  select(-rank)
        
knitr::kable(table_aisle, 
            caption = "Most popular items from Baking Ingredients, Dog Food Care, and Packaged Vegetables and Fruit")
```

# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)

note: I rounded mean hour to closest hour
#Q: still dont know if I should leave order_dow as is or covert to day of week w mutate (code is b) 
```{r}
mean_table = 
  instacart %>% 
  filter( product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  select( product_name, order_dow, order_hour_of_day) %>% 
  group_by( product_name, order_dow) %>% 
  summarize(mean = round(mean(order_hour_of_day))) %>% 
  pivot_wider(
    names_from = order_dow, 
    values_from = mean)

#mutate(order_dow = recode(as.character(order_dow), `1` = "Monday", `2` = "Tuesday")) %>%

knitr::kable(mean_table, 
            caption = "Mean Hour of Day for Orders of Pink Lady Apples and Coffee Ice Cream")
```



## Problem 2 

# Q: format the data to use appropriate variable names? - copy and paste what he has? variable names seem fine but last two are diff -  i changed locationabbr and locationdesc to state and country but maybe should use what he does 

```{r}
# var names  
var_names = GET("https://chronicdata.cdc.gov/views/acme-vg9e.json") %>%
  content("text") %>%
  fromJSON() %>% 
  .[["columns"]] %>% 
  .[["name"]] %>% 
  .[-23]

brfss_smart2010 = 
  GET("https://chronicdata.cdc.gov/views/acme-vg9e/rows.json") %>% 
  content("text") %>%
  fromJSON() %>% 
  .[["data"]]

row_as_tibble = function(row_as_list, var_names) {
  var_list = row_as_list[9:30]
  names(var_list) = var_names 
  var_list[sapply(var_list, is.null)] <- NULL
  as_tibble(var_list, validate = FALSE)
}

brfss_smart2010 = 
  brfss_smart2010 %>% 
  map(.x = ., ~row_as_tibble(.x, var_names)) %>% 
  bind_rows
```

my work (not using his code)
```{r}

# data cleaning
brfss = 
  janitor::clean_names(brfss_smart2010) %>% 
  separate(locationdesc, 
           into = c("state", "county"), 
           sep = " - ") %>% 
  select(- locationabbr) %>% 
  filter( topic == "Overall Health",
          response %in% c("Poor", "Fair","Good", "Very good", "Excellent")) %>% 
   mutate(response = forcats::fct_relevel(response, c("Poor", "Fair","Good", "Very good", "Excellent")))

```

# COMMENT on the results of each:

# In 2002, which states were observed at 7 or more locations? What about in 2010?

# Q: county or state? 

```{r}
# states at 7 or more locations

# 2002
brfss_2002 = 
  brfss %>% 
  filter( year == 2002) %>% 
  group_by(state) %>% 
  summarize( n_locations = n()) %>% 
  filter( n_locations >= 7)

# 2010
brfss_2010 = 
  brfss %>% 
  filter( year == 2010) %>% 
  group_by(state) %>% 
  summarize( n_locations = n()) %>% 
  filter( n_locations >= 7)
```

In 2002, the states `r print(pull(brfss_2002, state))` were observed at 7 or more location and in 2010, 45 states were observed at 7 or more locations, specifically, `r pull(brfss_2010, state)`.  

# Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state.
# Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
brfss_excellent = 
  brfss %>% 
  filter( response == "Excellent") %>% 
  group_by(year, state) %>% 
  summarize(avg_data_value = mean(data_value,  na.rm = TRUE)) 

brfss_excellent %>% 
  ggplot(aes( x = year, y = avg_data_value)) + 
  geom_line(aes(group = state)) +
  labs(
    title = "Mean data value for each state from 2002 to 2010",
    x = "Year",
    y = "Mean Data Value") +
  theme(axis.text.x = element_text(angle = 90),
        plot.title = element_text(hjust = 0.5))

# if want color too (not good i think)
brfss_excellent %>% 
  ggplot(aes( x = year, y = avg_data_value, color = state)) + 
  geom_line(aes(group = state)) + 
  theme(legend.position = "right") +
  labs(
    title = "Mean data value for each state from 2002 to 2010",
    x = "Year",
    y = "Mean Data Value") +
  theme(axis.text.x = element_text(angle = 90),
        plot.title = element_text(hjust = 0.5))
```

Commont: 
The “spaghetti” plot of average value over time within a state shows how varied the mean data value each year can be for the majority of states. Although adding color is not helpful in identifying which states belongs to each line, it helps show how the lines are frequently crossing each other, further depicting the up and down nature of average values for each state. 

# Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

# Q: could do boxlot or density 

```{r}
# plot 1 (2006) - density
plot_2006 = 
  brfss %>% 
    filter( year == 2006, state == "NY") %>% 
    ggplot(aes(x = data_value, fill = response)) +
    geom_density(alpha = .5) +
    labs(
      title = "Data Value in 2006 by Response",
      x = "Response",
      y = "Data Value") +
    viridis::scale_fill_viridis(discrete = TRUE) +
  theme(legend.position = "none")

# plot 2 (2010) 
plot_2010 = 
  brfss %>% 
    filter( year == 2010, state == "NY") %>% 
    ggplot(aes(x = data_value, fill = response)) +
    geom_density(alpha = .5) +
    labs(
      title = "Data Value in 2010 by Response",
      x = "Response",
      y = "Data Value") +
    viridis::scale_fill_viridis(discrete = TRUE)

# panel plots - density
plot_2006 + plot_2010

# panel plots - boxplot
boxplot_2006 + boxplot_2010
```

no panel - instead facet 
```{r}
# facet by year, density plot
brfss %>% 
  filter( year %in% c(2006, 2010)) %>% 
  ggplot(aes(x = data_value, fill = response)) +
  geom_density(alpha = .5) + 
  facet_grid(~year) +
  labs(
      title = "Density of Data Value by Response",
      x = "Response",
      y = "Data Value") + 
  theme(plot.title = element_text(hjust = 0.5))
```

By looking at the distribution of responses from 2006 and 2010 next to one another, we can see that for each response type, data values have very similar distributions. The most difference we see if how sharp a peak is, meaning the are most observed data value at one specific point, but the peaks for each response still seem to be around the same data values. To compare the medians and other specific values we can look at a boxplot too. 

```{r}
brfss %>% 
  filter( year %in% c(2006, 2010)) %>% 
  ggplot(aes(x = response, y = data_value, color = response)) + 
    geom_boxplot() + 
  facet_grid(~year) +
  labs(
      title = "Boxplot of Data Value by Response",
      x = "Response",
      y = "Data Value") + 
  theme(plot.title = element_text(hjust = 0.5))
```

The boxplots also shows us that apart from data values on the tails of each distribution, the center spread of data values is very similar in 2006 and 2010 for each response level. 

## Problem 3

# Load, tidy, and otherwise wrangle the data. 
-- Your final dataset should include all originally observed variables and values; (?)- so dont tidy?? does this mean keep the variables as is?
-- have useful variable names; 
-- include a weekday vs weekend variable; 
-- and encode data with reasonable variable classes. 
-- Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

way 1: (keeping original variables)
```{r}
accel_1 = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(day_type = replace( day , 
                             day %in% c("Saturday", "Sunday"),
                             "weekend"), 
         day_type = replace( day_type , 
                             day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"),
                             "weekday")) %>% 
  select( week, day_id, day, day_type, everything())
```

way 2: tidy to long 
```{r}
accel_2 = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_num", 
    values_to = "activity_count", 
    names_prefix = "activity_") %>% 
  mutate( minute_num = as.numeric(minute_num), 
          day_type = replace( day , 
                             day %in% c("Saturday", "Sunday"),
                             "weekend"), 
         day_type = replace( day_type , 
                             day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"),
                             "weekday")) %>% 
  select( week, day_id, day, day_type, everything())
```

# DESCRIBE DATASET (resulting dataset (e.g. what variables exist, how many observations, etc))


# Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

can do for accel 2 but not 1
```{r}
# make table wide?
table_total_activity = 
  accel_2 %>% 
  group_by(day_id) %>% 
  summarize( total_activity = sum(activity_count))

# look for trends
accel_2 %>% 
  group_by(day_id) %>% 
  summarize( total_activity = sum(activity_count)) %>% 
  ggplot(aes(x = day_id, y = total_activity)) +
  geom_point()
```

# Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

# need to make minutes into hours 
```{r}
accel_2 %>% 
  ggplot(aes(x = minute_num, y = activity_count, color = day)) +
  geom_point() +
  geom_line(aes(group = day_id))

accel_2 %>% 
  mutate(hour_conv = minute_num/60, 
         hour = rep(rep(0:23, each = 60), each = 35)) %>% 
  ggplot(aes(x = hour, y = activity_count, color = day)) +
  geom_point() +
  geom_line(aes(group = day_id))
  
  
  
  mutate(hour = replace( hour_conv , 
                             hour_conv < 2, 1 %in% c("Saturday", "Sunday"),
                             "weekend"), 
         day_type = replace( day_type , 
                             day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"),
                             "weekday")) 
```

# COMMENT 
